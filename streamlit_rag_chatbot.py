# streamlit_rag_chatbot.py

import os
from io import BytesIO
from dotenv import load_dotenv

import streamlit as st

import openai
import numpy as np
import faiss
from PyPDF2 import PdfReader

# ---------------------------------------
# 1Ô∏è‚É£ Chargement des variables d‚Äôenvironnement
# ---------------------------------------
load_dotenv()

# Renommage √©ventuel pour compatibilit√©
endpoint = os.getenv("AZURE_OPENAI_ENDPOINT") or os.getenv("AZURE_OPENAI_ENDPOINT_OPENAI")
api_key  = os.getenv("AZURE_OPENAI_KEY")
embed_model = os.getenv("AZURE_OPENAI_EMBED_MODEL", "text-embedding-3-large")
chat_model  = os.getenv("AZURE_OPENAI_CHAT_MODEL", "gpt-4o-mini")

if not endpoint or not api_key:
    st.error(
        "‚ö†Ô∏è Veuillez d√©finir dans votre `.env` :\n"
        "- AZURE_OPENAI_ENDPOINT (ou AZURE_OPENAI_ENDPOINT_OPENAI)\n"
        "- AZURE_OPENAI_KEY\n"
        "- AZURE_OPENAI_EMBED_MODEL\n"
        "- AZURE_OPENAI_CHAT_MODEL"
    )
    st.stop()

# ---------------------------------------
# 2Ô∏è‚É£ Configuration du SDK OpenAI pour Azure
# ---------------------------------------
openai.api_type    = "azure"
openai.api_base    = endpoint
openai.api_key     = api_key
openai.api_version = "2024-12-01-preview"

# ---------------------------------------
# 3Ô∏è‚É£ Initialisation de l‚Äôhistorique de la conversation
# ---------------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "system",
            "content": (
                "Vous √™tes un analyste de donn√©es capable d'analyser les documents et qui r√©pond aux questions "
                "en vous basant sur un document fourni."
            )
        }
    ]

# ---------------------------------------
# 4Ô∏è‚É£ Configuration de la page Streamlit
# ---------------------------------------
st.set_page_config(page_title="Chatbot RAG Azure OpenAI", layout="wide")
st.title("üìÑüîé Chatbot RAG avec Azure OpenAI")

# ---------------------------------------
# 5Ô∏è‚É£ T√©l√©versement du document PDF
# ---------------------------------------
uploaded_file = st.file_uploader(
    "1. T√©l√©versez un document PDF",
    type=["pdf"],
    help="Le PDF sera d√©coup√©, embeddi et index√© pour le chat RAG."
)

if uploaded_file:
    # Lecture et extraction du texte
    pdf_bytes = uploaded_file.read()
    reader    = PdfReader(BytesIO(pdf_bytes))
    pages     = [page.extract_text() or "" for page in reader.pages]
    full_text = "\n\n".join(pages).strip()
    word_count = len(full_text.split())
    st.success(f"Document charg√© ‚Äî {word_count} mots extraits.")

    # D√©coupage en chunks (~1000 caract√®res)
    CHUNK_SIZE = 1000
    chunks = [
        full_text[i : i + CHUNK_SIZE]
        for i in range(0, len(full_text), CHUNK_SIZE)
        if full_text[i : i + CHUNK_SIZE].strip()
    ]
    st.info(f"{len(chunks)} chunks cr√©√©s pour l‚Äôindex RAG.")

    # ---------------------------------------
    # 6Ô∏è‚É£ Calcul des embeddings & index FAISS
    # ---------------------------------------
    progress = st.progress(0)
    embeddings = []
    for idx, chunk in enumerate(chunks, start=1):
        resp = openai.embeddings.create(
            model=embed_model,
            input=chunk
        )
        embeddings.append(resp.data[0].embedding)
        progress.progress(idx / len(chunks))
    progress.empty()

    emb_dim = len(embeddings[0])
    index   = faiss.IndexFlatL2(emb_dim)
    index.add(np.array(embeddings, dtype="float32"))

    # ---------------------------------------
    # 7Ô∏è‚É£ Interface de chat
    # ---------------------------------------
    st.subheader("2. Posez votre question")
    question = st.text_input("Votre question ici‚Ä¶")

    if st.button("Envoyer"):
        # Embedding de la question
        q_resp = openai.embeddings.create(
            model=embed_model,
            input=question
        )
        q_emb = q_resp.data[0].embedding

        # Recherche des 5 chunks les plus similaires
        D, I = index.search(np.array([q_emb], dtype="float32"), k=5)
        context = "\n\n---\n\n".join(chunks[i] for i in I[0])

        # Construction du prompt RAG
        rag_content = (
            f"Voici des passages extraits du document :\n\n{context}\n\n"
            f"Question : {question}\n\nR√©ponse :"
        )
        st.session_state.messages.append({"role": "user", "content": rag_content})

        # ‚Äî Correction ici : utilisation de `model=` au lieu de `deployment_id=` ‚Äî
        chat_resp = openai.chat.completions.create(
            model=chat_model,
            messages=st.session_state.messages
        )
        answer = chat_resp.choices[0].message.content
        st.session_state.messages.append({"role": "assistant", "content": answer})

    # ---------------------------------------
    # 8Ô∏è‚É£ Affichage de la conversation
    # ---------------------------------------
    st.subheader("üí¨ Conversation")
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            st.chat_message("user").write(msg["content"])
        else:
            st.chat_message("assistant").write(msg["content"])
